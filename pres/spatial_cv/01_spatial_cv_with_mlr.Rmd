---
title: "Geocomputation with R"
subtitle: "⚔<br/>Spatial cross-validation with **mlr**"
author: "Jannes Muenchow"
date: "GeoStats 2018"
output:
  xaringan::moon_reader:
    css: ../../xaringan_stuff/my-theme.css
    seal: true
    lib_dir: ../../xaringan_stuff/libs
    nature:
#      highlightStyle: dracula
      highlightLines: true
      ratio: '4:3'
      countIncrementalSlides: false
---

```{r setup, include = FALSE}
options(htmltools.dir.version = FALSE)
library(RefManageR)
BibOptions(check.entries = FALSE, 
           bib.style = "authoryear", 
           cite.style = 'alphabetic', 
           style = "markdown",
           first.inits = FALSE,
           hyperlink = FALSE, 
           dashed = FALSE)
my_bib = ReadBib("../../xaringan_stuff/references.bib", check = FALSE)
```


layout: true
background-image: url(../../xaringan_stuff/img/r_geocomp_background.png)
background-size: cover

---

# Find the slides and code
<br>
<br>
<br>
<br>
https://github.com/geocompr/geostats_18

<br>
<br>
Please install following packages:
```{r, eval=FALSE}
list_of_packages = c("sf", "raster", "mlr", "ranger", "RQGIS",
                     "parallelMap")
install.packages(list_of_packages)
```


---
layout: false

# Contents of the tutorial

```{r, eval=FALSE, echo=FALSE}
library(sf)
library(spData)
world_laea = st_transform(world, 
                          crs = "+proj=laea +x_0=0 +y_0=0 +lon_0=-77 +lat_0=39")
gr = st_graticule(ndiscr = 1000) %>%
  st_transform("+proj=laea +y_0=0 +lon_0=-77 +lat_0=39 +ellps=WGS84 +no_defs")
png(filename = "pres/img/globe.png")
plot(gr$geometry, col = "lightgray", lwd = 3)
plot(world_laea$geom, bg = "white", col = "lightgray", add = TRUE)
dev.off()
```

<figure>
<img align="right" src="../../xaringan_stuff/img/globe.png" width = "60%", height = "60%"/>
</figure>


1. Study area, data and aim
--

1. Introduction to **(spatial) cross-validation**
--

1. **mlr** building blocks
--

1. Random forest modeling with **mlr**
--

---

layout: true
background-image: url(../../xaringan_stuff/img/r_geocomp_background.png)
background-size: cover

---
class: inverse, center, middle

# Study area, data and aim

---

# Study area

Where are we? Mount Mongón near Casma in northern Peru.
<center>
<figure>
<img align="center" src="img/study_area.png", width="60%", height="60%"/>
</figure>
</center>
---
# Austral summer

<center>
<figure>
<img align="center" src="img/desert_near_mongon.png", width="100%", height="100%"/>
</figure>
</center>
---

# Mount Mongón in summer

<center>
<figure>
<img align="center" src="img/mongon_summer.png"/>
</figure>
</center>

---

# Mount Mongón in austral winter

<center>
<figure>
<img align="center" src="img/mongon_winter.png", width=90%, height=90%/>
</figure>
</center>
---

# Data

.pull-left[
- 100 randomly distributed plots
- coverage of all vascular plants in each plot
- First NMDS axis represents the main gradient (our response, see Chapter 14 of [*Geocomputation with R*](https://geocompr.robinlovelace.net/eco) `r Citep(my_bib, "lovelace_geocomputation_2018", .opts = list(cite.style = "authoryear"))`
]


.pull-right[
```{r, echo=FALSE, message = FALSE}
library("latticeExtra")
library("grid")
library("sf")
library("raster")

# attach the data
data("study_area", "random_points", "comm", "dem", "ndvi", package = "RQGIS")
# create hillshade
hs = hillShade(terrain(dem), terrain(dem, "aspect"))
p_1 = spplot(dem, col.regions = terrain.colors(50), alpha.regions = 0.5,
       scales = list(draw = TRUE,
                     tck = c(1, 0)),
       colorkey = list(space = "right", title = "m asl",
                               width = 0.5, height = 0.5,
                       axis.line = list(col = "black")),
       sp.layout = list(
         list("sp.points", as(random_points, "Spatial"), pch = 16,
              col = "black", cex = 1.25, first = FALSE),
         list("sp.polygons", as(study_area, "Spatial"), 
              col = "black", lwd = 1.5, first = FALSE)
       )
       ) + 
  latticeExtra::as.layer(spplot(hs, col.regions = gray(0:100 / 100)), 
                         under = TRUE)
print(p_1)
grid.text("m asl", x = unit(0.95, "npc"), y = unit(0.75, "npc"), 
          gp = gpar(cex = 1))
```
]

---

# Aim

.pull-left[
- model the floristic gradient as a function of environmental predictors using a random forest model
- spatial cross-validation to retrieve a bias-reduced estimate of the model's performance
- tune hyperparameters for the predictive mapping of the floristic gradient
- but before we do that, we will introduce the **mlr** building blocks to easily do spatial cross-validation with a simple `lm` (though this definitely is not be the most appropriate model for our data...)
]

.pull-right[
```{r, echo=FALSE, message = FALSE}
print(p_1)
grid.text("m asl", x = unit(0.95, "npc"), y = unit(0.75, "npc"), 
          gp = gpar(cex = 1))
```
]


---
class: inverse, center, middle

# Introduction to (spatial) cross-validation

---

# Cross-validation

The aim of (spatial) cross-validation is to find out how generalizable a model is.
Fitting to closely the input data, including its noise, leads to a bad predictive performance on unseen data (overfitting).

---

# Random partitioning

<figure>
<img align="center" src="img/random_partitioning.png"
</figure>

--

Problems with conventional random partitioning when using spatial data:

- violation of the fundamental independence assumption in cross-validation
- which subsequently leads to overoptimistic, i.e. biased results
- Solution: use spatial partitioning for a bias-reduced assessment of a **model's performance**

---

# Spatial partitioning


<figure>
<img align="center" src="img/spatial_partitioning.png">
</figure>


---
class: inverse, center, middle

# **mlr** building blocks

---

# Input data
```{r, echo=FALSE, message=FALSE}
library(mlr)
library(dplyr)
library(sf)
rp = readRDS("../../code/spatial_cv/images/rp.rds")
# extract the coordinates into a separate dataframe
coords = sf::st_coordinates(rp) %>%
  as.data.frame %>%
  rename(x = X, y = Y)
# only keep response and predictors which should be used for the modeling
rp = dplyr::select(rp, -id, -spri) %>%
  st_set_geometry(NULL)
```

We are already in possession of the following data

```{r, message=FALSE}
library(mlr)
library(dplyr)
library(sf)
# response-predictor dataframe
head(rp, 2)
# coordinates
head(coords, 2)
```

---

# Little data exploration

<center>
```{r, echo=FALSE, out.width="65%"}
# first have a look at the data
d = reshape2::melt(rp, id.vars = "sc")
xyplot(sc ~ value | variable, data = d, pch = 21, fill = "lightblue", cex = 1.5,
       col = "black", ylab = "response (sc)", xlab = "predictors",
       scales = list(x = "free", 
                     tck = c(1, 0),
                     alternating = c(1, 0)),
       strip = strip.custom(bg = c("white"),
                            par.strip.text = list(cex = 1.2)),
       panel = function(x, y, ...) {
         panel.points(x, y, ...)
         panel.loess(x, y, col = "salmon", span = 0.5, lwd = 2)
       })
```
</center>

---

# Building blocks

**mlr** is a metapackage that lets you combine hundreds of modeling algorithms of many different packages within a single framework `r Citep(my_bib, "bischl_mlr:_2016", .opts = list(cite.style = "authoryear"))`

<center>
<figure>
<img align="center" src="img/ml_abstraction_crop.png" width="90%" height="90%">
</figure>
</center>

Source: [openml.github.io](http://openml.github.io/articles/slides/useR2017_tutorial/slides_tutorial_files/ml_abstraction-crop.png)
---

# Create a task
```{r}
library(mlr)
# create task
task = makeRegrTask(data = rp, target = "sc",
                    coordinates = coords)
```

---

# Learner

To find out which learners are available for a specific task, run:

```{r, eval=FALSE}
lrns = listLearners(task, warn.missing.packages = FALSE)
dplyr::select(lrns, class, name, short.name, package)
```

--
We already know that there is a learner named `regr.lm` for running a simple linear model.

---

# Define the learner

```{r}
lrn = makeLearner(cl = "regr.lm", predict.type = "response")
```

--
To find out more about the learner, run:

```{r, eval=FALSE}
# simple lm of the stats package
getLearnerPackages(lrn)
helpLearner(lrn)
```

--
Just to convince you that we are really using a simple `lm`, let us retrieve the learner model:

```{r}
getLearnerModel(train(lrn, task))
```

---

# Define spatial partitioning
```{r}
# performance level
perf_level = makeResampleDesc(method = "SpRepCV", 
                              folds = 5, 
                              reps = 100)
```

--

<center>
<figure>
<img align="center" src="img/spatial_partitioning.png"/>
</figure>
</center>
---

# Execute the resampling
```{r, cache=TRUE}
cv_sp_lm = mlr::resample(
  task = task,
  learner = lrn,
  resampling = perf_level, 
  # specify the performance measure
  measures = mlr::rmse)
# boxplot(cv_sp$measures.test$rmse)
```
???
```{r, eval=FALSE, echo=FALSE}
# we can run the same using a conventional cross-validation
# task_nsp = makeRegrTask(data = rp, target = "sc")
# perf_level_nsp = makeResampleDesc(method = "RepCV", folds = 5, reps = 100)
# cv_nsp_lm = mlr::resample(learner = lrn, 
#                       task = task_nsp,
#                       resampling = perf_level_nsp, 
#                       measures = mlr::rmse)
# boxplot(cv_sp_lm$measures.test$rmse, cv_nsp_lm$measures.test$rmse,
#         col = c("lightblue2", "mistyrose2"),
#         names = c("spatial CV", "conventional CV"),
#         ylab = "RMSE")
```
---

# Have a look at the result
```{r}
cv_sp_lm
```

Ok, is this good or not?
--
```{r}
range(rp$sc)
```
--

Hence, this corresponds to a mean deviation from the true value of (%):

```{r}
cv_sp_lm$aggr / diff(range(rp$sc)) * 100
```
???
Admittedly, better than expected... but can we do better?
---
class: inverse, center, middle

# Random forests

---

# Random forests
Like many other machine learning algorithms, random forests have hyperparameters `r Citep(my_bib, "james_introduction_2013", .opts = list(cite.style = "authoryear"))`. 
These hyperparameters are not estimated from the data like the coefficients of (semi-)parametric models (`lm`, `glm`, `gam`) but need to be specified before the learning begins. 
To find the optimal hyperparameters, one needs to run many models using random hyperparameter values.
There are several approaches how to do this, here, we will use a random search with 50 iterations while we limit the tuning space to a specific range in accordance with the literature `r Citep(my_bib, "probst_hyperparameters_2018","schratz_performance_2018", .opts = list(cite.style = "authoryear"))`.

---
# Again: Define a learner
We can use the already specified regression task... just in case let us repeat it here again

```{r}
task = makeRegrTask(data = rp, target = "sc",
                    coordinates = coords)
```

--
But we need to change our learner in order to use a random forest model.
Here, we will use a random forest implementation of the **ranger** package, i.e. we replace `regr.lm` by `reg.ranger` (again see `listLearners(task)`).

--

```{r}
lrn = makeLearner(cl = "regr.ranger", predict.type = "response")
```


---

# Spatial cross-validation

We are already familiar with the spatial cross-validation of the performance level (outer level).
```{r}
perf_level = makeResampleDesc(method = "SpRepCV", folds = 5,
                              reps = 100)
```

---

# Hyperparameter tuning
However, now that we use a random forest model, we need to tune its hyperparameters. 
And we have to do it in an inner loop using again spatial cross-validation.
The inner loop is necessary because tuning the hyperparameters in the performance loop would be like cheating a bit since we then would use the same data for the performance estimation and the hyperparameter tuning.
This is called nested spatial cross-validation.
A visualization might help (taken from `r Citet(my_bib, "schratz_performance_2018", .opts = list(cite.style = "authoryear"))`):

---

<br>
<br>
<br>

<center>
<figure>
<img align="center" src="img/cv.png" width="150%" height="150%"/>
</figure>
</center>

--

I know this might seem a bit overwhelming in the beginning but it is an easy concept once you get your head around it.
You can reread nested spatial cross-validation including hyperparamter tuning in Chapter 11 of [*Geocomputation with R*](https://geocompr.robinlovelace.net/spatial-cv) `r Citep(my_bib, "lovelace_geocomputation_2018", .opts = list(cite.style = "authoryear"))`

???
Model selection without nested CV uses the same data to tune model parameters and evaluate model performance. Information may thus “leak” into the model and overfit the data. The magnitude of this effect is primarily dependent on the size of the dataset and the stability of the model. See Cawley and Talbot [1] for an analysis of these issues.
http://scikit-learn.org/stable/auto_examples/model_selection/plot_nested_cross_validation_iris.html


Since we used the test set to both select the values of the parameter and evaluate the model, we risk optimistically biasing our model evaluations. For this reason, if a test set is used to select model parameters, then we need a different test set to get an unbiased evaluation of that selected model.

One way to overcome this problem is to have nested cross validations. First, an inner cross validation is used to tune the parameters and select the best model. Second, an outer cross validation is used to evaluate the model selected by the inner cross validation.

---

# Hyperparameter tuning

Let us define five spatially disjoint partitions in the tune level (one repetition).

```{r}
tune_level = makeResampleDesc(method = "SpCV", iters = 5)
```

---

# Random search  

Next, we need to tell **mlr** to find the optimal hyperparameters via a random search with 50 iterations:

```{r}
ctrl = makeTuneControlRandom(maxit = 50)
```

--
Let us limit the tuning space in accordance with the literature `r Citep(my_bib, "probst_hyperparameters_2018", .opts = list(cite.style = "authoryear"))` 

```{r}
ps = makeParamSet(
  makeIntegerParam("mtry", lower = 1, upper = ncol(rp) - 1),
  makeNumericParam("sample.fraction", lower = 0.2, upper = 0.9),
  makeIntegerParam("min.node.size", lower = 1, upper = 10)
)
```

--
Recommended literature:

- `r Citet(my_bib, "james_introduction_2013", .opts = list(cite.style = "authoryear"))`
- `r Citet(my_bib, "probst_hyperparameters_2018", .opts = list(cite.style = "authoryear"))`. 
- Chapter 14 of [*Geocomputation with R*](https://geocompr.robinlovelace.net/eco) `r Citep(my_bib, "lovelace_geocomputation_2018", .opts = list(cite.style = "authoryear"))`


???
In random forests, the hyperparameters mtry, min.node.size and sample.fraction determine the degree of randomness, and should be tuned (Probst, Wright, and Boulesteix 2018). 
**mtry** indicates how many predictor variables should be used in each tree. If all predictors are used, then this corresponds in fact to bagging (see beginning of section 14.4).
The **sample.fraction** parameter specifies the fraction of observations to be used in each tree. Smaller fractions lead to greater diversity, and thus less correlated trees which often is desirable (see above). 
The **min.node.size** parameter indicates the number of observations a terminal node should at least have (see also Figure 14.4). Naturally, trees and computing time become larger, the lower the min.node.size.
---

# Wrap it all up

```{r, eval=FALSE}
wrapped_lrn_rf = 
  makeTuneWrapper(learner = lrn_rf,
                  # inner loop (tunning level)
                  resampling = tune_level,
                  # hyperparameter seach space
                  par.set = ps,
                  # random search
                  control = ctrl,
                  show.info = TRUE,
                  # performance measure
                  measures = mlr::rmse)
```

---

# Resampling

Be careful, running the next code chunk takes a while since we are asking R to run `125,500` models.
Parallelization might be a good idea. 
See `code/spatial_cv/01-mlr.R` of the `geocompr/geostats_18` repository how to set it up. 

```{r, eval=FALSE}
set.seed(12345)
cv_sp_rf = mlr::resample(learner = wrapped_lrn_rf,
                         task = task,
                         resampling = perf_level,
                         extract = getTuneResult,
                         measures = mlr::rmse)
```

---

# Have a look at the result


```{r, echo=FALSE, fig.align="center"}
# attach data again
cv_sp_rf = readRDS("../../code/spatial_cv/images/rf_sp_sp_50it.rds")
cv_nsp_rf = readRDS("../../code/spatial_cv/images/rf_nsp_nsp_50it.rds")

# Difference between the mean (-> heavily influenced by outliers)
# still, this is a strong indication that nsp cv produced over-optimistic
# results
# result$aggr
# result_nsp$aggr
# Visualize difference between spatial and non-spatial CV
boxplot(cv_sp_rf$measures.test$rmse,
        cv_nsp_rf$measures.test$rmse,
        col = c("lightblue2", "mistyrose2"),
        names = c("spatial CV", "conventional CV"),
        ylab = "RMSE")
``` 

---
# Interesting...

```{r}
cv_sp_lm$aggr
cv_sp_rf$aggr
```

--
The linear model is better than the random forest model...

--
Reasons:
- Relationship between response and predictors is linear enough
- just four predictors, adding further environmental predictors and xy-coordinates might change the result

???
In fact, one often uses random forests with many, many variables.
Still, it is always worth to have a look at simple solutions -> use the most parsimonious model

---

# Predictive mapping
Find the code again in `code/spatial_cv/01-mlr.R` of the `geocompr/geostats_18` respository.

<center>
<figure>
<img align="center" src="img/pred_map.png", width=60%, height=60%/>
</figure>
</center>


---
class: small
# References

```{r, 'refs', results="asis", echo=FALSE}
PrintBibliography(my_bib)
```

---
layout: true
background-image: url(img/fog_flag_mongon.png)
background-size: cover

---

